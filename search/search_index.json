{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ISCC - Evaluation","text":"<p>Evaluating the International Standard Content Code (ISCC)</p> <p>A Comprehensive and Scientific Approach to Understand the Capabilities of ISCC</p> <p>ISCC Status</p> <p>As of July 2023 the International Standard Content Code is not yet a published ISO standard. ISO/CD 24138.2 is under development at TC 46/SC 9/WG 18. https://www.iso.org/standard/77899.html</p>"},{"location":"#digital-content-identification","title":"Digital Content Identification","text":"<p>Welcome to the exploration and evaluation of the International Standard Content Code (ISCC). In the digital landscape, accurately identifying and cataloguing content is a critical task. The ISCC is designed to address this need as a versatile, universal identifier for digital content, encompassing a broad spectrum of formats like text, image, audio, and video.</p>"},{"location":"#understanding-iscc","title":"Understanding ISCC","text":"<p>With any innovative technology comes a learning curve, and the ISCC is no exception. Across various disciplines, the ISCC's unique approach has sparked many questions:</p> <ul> <li>Registration agencies are seeking to understand how this differs from established standard   identifiers.</li> <li>Content recognition specialists are interested in its capabilities in granular content matching.</li> <li>Software developers, accustomed to cryptographic hashes for secure data identification, are   curious about how the ISCC compares.</li> </ul> <p>Our aim is to provide clear insights into the capabilities of the ISCC, dispel any misconceptions, and deliver a well-rounded understanding of this new and exciting technology.</p>"},{"location":"#introducing-twinspect","title":"Introducing TwinSpect","text":"<p>Purpose</p> <p>The primary purpose of TwinSpect is to assess to which extent the ISCC is capable of clustering and matching similar content.</p> <p>To address concerns and provide clarity, we've developed TwinSpect \u2014 a comprehensive open-source framework engineered for evaluating the ISCC in an accessible and scientifically robust manner. TwinSpect is built on widely recognized metrics from the field of information retrieval and makes use of various public and private datasets selected and built to fit our evaluation objectives.</p> <p>While TwinSpect is specifically designed to evaluate the ISCC, its adaptability goes beyond. TwinSpect can easily be extended with custom algorithms, datasets, transformations and metrics.</p>"},{"location":"#iscc-ai","title":"ISCC &amp; AI","text":"<p>As we enter the era of Artificial Intelligence, where content creation and distribution are becoming increasingly automated, the role of the ISCC in ensuring accurate content identification, and improving the trust in content provenance becomes even more critical.</p> Upcoming Semantic-Code <p>As of July 2023 the ISO draft of the upcoming ISCC Standard already reserves prefixes for the ISCC Semantic-Code which will employ Deep Learning and Artificial Intelligence techniques to create ISCC-UNITs that match similarity based on the high-level understanding of concepts. For example the Semantic-Code for textual content will be capable of creating similar codes for the same text translated to different languages (cross-lingual similarity matching).</p>"},{"location":"#what-s-ahead","title":"What\u00b4s Ahead","text":"<p>In the following pages, we will provide a detailed look at the ISCC, explain the TwinSpect evaluation framework, and share the findings from our evaluation. Our goal is to give you a thorough understanding of the ISCC and its potential, in a clear, concise, and business-focused manner.</p>"},{"location":"about/","title":"About ISCC","text":"<p>...</p>"},{"location":"algorithms/","title":"Evaluated Algorithms","text":""},{"location":"algorithms/#iscc-text-code-v0","title":"ISCC Text-Code V0","text":"<p>The ISCC Text-Code is similarity preserving hash designed to cluster and match near-duplicate text documents that have undergone format conversion or minor edits.</p> <p>The reference implementation is available in the <code>iscc-core</code> GitHub Repository</p>"},{"location":"algorithms/#iscc-image-code-v0","title":"ISCC Image-Code V0","text":"<p>The ISCC Image-Code is similarity preserving perceptual hash designed to cluster and match near-duplicate images that have undergone format conversion or minor edits.</p> <p>The reference implementation is available in the <code>iscc-core</code> GitHub Repository</p>"},{"location":"algorithms/#iscc-audio-code-v0","title":"ISCC Audio-Code V0","text":"<p>The ISCC Audio-Code is similarity preserving hash based on chromaprint and designed to cluster and match near-duplicate audio files that have undergone format conversion, transcoding, compression and other minor edits.</p> <p>The reference implementation is available in the <code>iscc-core</code> GitHub Repository</p>"},{"location":"algorithms/#iscc-video-code-v0","title":"ISCC Video-Code V0","text":"<p>The ISCC Video-Code is similarity preserving hash based on the MPEG-7 Video Signature and is designed to cluster and match near-duplicate vides that have undergone format conversion or minor edits.</p> <p>The reference implementation is available in the <code>iscc-core</code> GitHub Repository</p>"},{"location":"datasets/","title":"Datasets for ISCC Evaluation","text":""},{"location":"datasets/#introduction","title":"Introduction","text":""},{"location":"datasets/#dataset-types","title":"Dataset Types","text":"<p>To test the capabilities of the ISCC, we need a collection of test data, including ground truth data. Ground truth data are essentially benchmarks that provide us with a standard of truth. They help us measure the ISCC's accuracy in detecting similar or duplicate content. These benchmarks can be established in two ways:</p> <ol> <li>Real-world media file collections annotated with information about near-duplicates within the    dataset. These collections offer insights into the ISCC's performance in a real-world setting,    albeit contingent on the quality of annotations.</li> <li>Synthetically transformed media files. We can take a unique media file and apply various    modifications to it. This strategy tests the ISCC's resilience against different transformations,    although the synthetic changes might not fully mirror real-world variations.</li> </ol>"},{"location":"datasets/#data-folders","title":"Data Folders","text":"<p>To streamline processing and ensure comparability, our datasets adhere to a predefined directory structure and file naming convention. This approach implicitly encodes ground truth and optionally transformation information, simplifying the evaluation of an algorithm's performance against different datasets. Here's an example snapshot of our data folder structure:</p> <pre><code>data-folder\n\u251c\u2500\u2500 cluster1           # Collections of media files considered duplicates\n\u2502   \u251c\u2500\u2500 0original.mp3  # The first file is the original (lexicographic porder)\n\u2502   \u251c\u2500\u2500 mod1.mp3       # Variation that should match against other files in the cluster\n\u2502   \u251c\u2500\u2500 mod2_rot20.mp3 # Variation with named transform\n\u2502   \u2514\u2500\u2500 ...            # Create as many modified versions as you like\n\u251c\u2500\u2500 cluster2           # A cluster folder can have any name\n\u2502   \u251c\u2500\u2500 file1.mp3      # Files in cluster folders can have any name\n\u2502   \u2514\u2500\u2500 ...            # A cluster folder may have only one file (query with no match)\n\u251c\u2500\u2500 sample1.mp3        # Top-Level files that should NOT match against any other files\n\u251c\u2500\u2500 sample2.mp3        # The ratio of distractor content vs. cluster content is relevant for metrics\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/#evaluation-datasets","title":"Evaluation Datasets","text":"<p>Info</p> <p>The content of this section is autogenerated based on latest published configuration of the TwinSpect Benchmark.</p>"},{"location":"datasets/#stlib-2000","title":"STLIB-2000","text":"<p>Dataset Info</p> <ul> <li>ID: e20bd16e097e8faf</li> <li>Mode: Text</li> <li>Size: 4.8 GB</li> <li>Files: 1610</li> </ul> <p>The STLIB-2000 is a dataset, designed to assess the accuracy of text identification algorithms. It includes ground truth data for a total of 1610 text files with near-duplicates organized into 805 clusters.</p> <p>The STLIB-2000 is a real-world dataset of 2000 commercial E-Books where each title has an EPUB and PDF version. The data has been generously provided by StreetLib. Because the ISCC-SDK does not support OCR yet, titles with image-only E-Books have been removed before benchmarking.</p> Clustering Details <p>Each cluster contains 2 near-duplicate text files.</p>"},{"location":"datasets/#iscc-fma-10k","title":"ISCC-FMA-10K","text":"<p>Dataset Info</p> <ul> <li>ID: 142e3bd331044320</li> <li>Mode: Audio</li> <li>Size: 69.0 GB</li> <li>Files: 10000</li> </ul> <p>The ISCC-FMA-10K is a dataset, designed to assess the accuracy of audio identification algorithms. It includes ground truth data for a total of 10000 audio files with near-duplicates organized into 500 clusters.</p> <p>Additionally, the dataset contains 4500 unique audio files, with no corresponding duplicates within the set.</p> <p>The ISCC-FMA-10k benchmark is a subset of Free Music Archive Dataset. The subset is generated by collecting 5000 random audio files (longer than 60 seconds). Additionaly 10 synthetic transformations are applied to a random selection 500 of the audio files. The Twinspect benchmark automatically downloads and reproduces the tested dataset.</p> Clustering Details <p>Each cluster contains 11 near-duplicate audio files.</p> Synthetic Transformations <p>The following transformations were applied to 500 files of the dataset to simulate different conditions that might be encountered in real-world applications:</p> <ul> <li>equalize: Equalize audio (ffmpeg equalizer=f=1000:t=o:w=200:g=10)</li> <li>loudnorm: Apply loudness normalization (ffmpeg loudnorm=I=-16:TP=-1.5:LRA=11)</li> <li>fade-8s-both: Fade in/out 8 seconds at start and end</li> <li>trim-5s-both: Remove 5 seconds of audio from start and end</li> <li>transcode-aac-32kbps: Transcode audio to 32kbps AAC</li> <li>transcode-mp3-128kbps: Transcode audio to 128kbps MP3</li> <li>echo: Apply echo effect (ffmpeg aecho=0.8:0.7:60:0.2)</li> <li>trim-1s-both: Remove 1 seconds of audio from start and end</li> <li>transcode-ogg-64kbps: Transcode audio to 64kbps OGG</li> <li>compress-medium: Apply audio compression (attack 10, release 200, ratio 3, threshold -20)</li> </ul>"},{"location":"datasets/#mirflickr-mfnd","title":"MIRFLICKR-MFND","text":"<p>Dataset Info</p> <ul> <li>ID: 71d8a361044c7b5f</li> <li>Mode: Image</li> <li>Size: 520.0 MB</li> <li>Files: 4062</li> </ul> <p>The MIRFLICKR-MFND is a dataset, designed to assess the accuracy of image identification algorithms. It includes ground truth data for a total of 4062 image files with near-duplicates organized into 1958 clusters.</p> <p>The MFND benchmark (Connor et al., 2015) is a subset of the real-world MIRFLICKR dataset (Huiskes &amp; Lew, 2008) with annotations for near duplicates (IND). The Twinspect benchmark automatically downloads and reproduces the tested dataset.</p> Clustering Details <p>Clusters contain an average of 2.07 near-duplicate image files.</p> <p>Cluster sizes</p> <ul> <li>Minimum: 1</li> <li>Maximum: 14</li> <li>Mean: 2.07</li> <li>Median: 2.0</li> </ul>"},{"location":"results/","title":"Evaluation Results","text":"<p>Info</p> <p>The content of this section is autogenerated based on latest published configuration of the TwinSpect Benchmark.</p>"},{"location":"results/#overview","title":"Overview","text":"<p>Effectiveness of all algorithm/dataset pairs at optimum F1-Score:</p> Algorithm Dataset Threshold Recall Precision F1-Score AUDIO-CODE-64 ISCC-FMA-10K 4 0.87 0.89 0.88 IMAGE-CODE-64 MIRFLICKR-MFND 12 0.91 0.96 0.94 TEXT-CODE-64 STLIB-2000 11 0.98 0.97 0.98"},{"location":"results/#audio-code-64","title":"AUDIO-CODE-64","text":"<p>Evaluation against dataset ISCC-FMA-10K</p>"},{"location":"results/#effectiveness","title":"Effectiveness","text":"Understanding the Effectiveness Chart <p>This chart evaluates the effectiveness of a similarity hash in comparing media files. Each hash is compared against all others at different distance thresholds, with results assessed against the ground truth.</p> <p>Chart Interpretation:</p> <ul> <li>The X-Axis shows \"Hamming Distance Query Thresholds\". Each threshold marks a maximum   distance for two hashes to be considered similar.</li> <li>The Y-Axis represents Recall, Precision, and F1-Score:</li> <li>Recall: The fraction of actual matches correctly identified by the hash. Higher recall   indicates better match detection.</li> <li>Precision: The ratio of correct predictions to the total number of predictions. Higher   precision implies more reliable predictions.</li> <li>F1-Score: Harmonic mean of Precision and Recall, balancing both measures. A high F1-score   signals an effective algorithm.</li> </ul> <p>The curves display how these metrics vary across thresholds.</p> <p></p>"},{"location":"results/#robustness","title":"Robustness","text":"Transformation Minimum Maximum Mean Median compress-medium 0 11 2.006 1.0 echo 0 15 3.598 3.0 equalize 0 12 1.214 1.0 fade-8s-both 0 4 0.436 0.0 loudnorm 0 5 0.55 0.0 transcode-aac-32kbps 0 10 1.786 1.0 transcode-mp3-128kbps 0 3 0.33 0.0 transcode-ogg-64kbps 0 7 0.864 1.0 trim-1s-both 0 5 0.936 1.0 trim-5s-both 0 11 2.4 2.0"},{"location":"results/#distribution","title":"Distribution","text":""},{"location":"results/#performance","title":"Performance","text":"<ul> <li>Minimum: 0.41 MB/s</li> <li>Maximum: 32.92 MB/s</li> <li>Mean: 6.51 MB/s</li> <li>Median: 5.61 MB/s</li> </ul>"},{"location":"results/#image-code-64","title":"IMAGE-CODE-64","text":"<p>Evaluation against dataset MIRFLICKR-MFND</p>"},{"location":"results/#effectiveness_1","title":"Effectiveness","text":"Understanding the Effectiveness Chart <p>This chart evaluates the effectiveness of a similarity hash in comparing media files. Each hash is compared against all others at different distance thresholds, with results assessed against the ground truth.</p> <p>Chart Interpretation:</p> <ul> <li>The X-Axis shows \"Hamming Distance Query Thresholds\". Each threshold marks a maximum   distance for two hashes to be considered similar.</li> <li>The Y-Axis represents Recall, Precision, and F1-Score:</li> <li>Recall: The fraction of actual matches correctly identified by the hash. Higher recall   indicates better match detection.</li> <li>Precision: The ratio of correct predictions to the total number of predictions. Higher   precision implies more reliable predictions.</li> <li>F1-Score: Harmonic mean of Precision and Recall, balancing both measures. A high F1-score   signals an effective algorithm.</li> </ul> <p>The curves display how these metrics vary across thresholds.</p> <p></p>"},{"location":"results/#distribution_1","title":"Distribution","text":""},{"location":"results/#performance_1","title":"Performance","text":"<ul> <li>Minimum: 0.27 MB/s</li> <li>Maximum: 3.52 MB/s</li> <li>Mean: 0.75 MB/s</li> <li>Median: 0.70 MB/s</li> </ul>"},{"location":"results/#text-code-64","title":"TEXT-CODE-64","text":"<p>Evaluation against dataset STLIB-2000</p>"},{"location":"results/#effectiveness_2","title":"Effectiveness","text":"Understanding the Effectiveness Chart <p>This chart evaluates the effectiveness of a similarity hash in comparing media files. Each hash is compared against all others at different distance thresholds, with results assessed against the ground truth.</p> <p>Chart Interpretation:</p> <ul> <li>The X-Axis shows \"Hamming Distance Query Thresholds\". Each threshold marks a maximum   distance for two hashes to be considered similar.</li> <li>The Y-Axis represents Recall, Precision, and F1-Score:</li> <li>Recall: The fraction of actual matches correctly identified by the hash. Higher recall   indicates better match detection.</li> <li>Precision: The ratio of correct predictions to the total number of predictions. Higher   precision implies more reliable predictions.</li> <li>F1-Score: Harmonic mean of Precision and Recall, balancing both measures. A high F1-score   signals an effective algorithm.</li> </ul> <p>The curves display how these metrics vary across thresholds.</p> <p></p>"},{"location":"results/#distribution_2","title":"Distribution","text":""},{"location":"results/#performance_2","title":"Performance","text":"<ul> <li>Minimum: 0.00 MB/s</li> <li>Maximum: 6.76 MB/s</li> <li>Mean: 0.21 MB/s</li> <li>Median: 0.07 MB/s</li> </ul>"},{"location":"similarity/","title":"Kinds of Similarity","text":"<p>Digital content similarity refers to the assessment of likeness between various types of digital media. For the purposes of the ISCC the concept of such similarity can be classified into three primary categories, each examining different aspects of the digital files: data similarity, content similarity, and semantic similarity.</p>"},{"location":"similarity/#1-data-similarity","title":"1. Data Similarity","text":"<p>Data Similarity:</p> <p>The measure of likeness between two digital media files, based on a direct comparison of their raw binary data, without considering the interpretation or meaning of the content.</p> <p>This type of similarity compares the raw, uninterpreted bitstreams of digital media files, assessing their likeness based on the sequence of bits and bytes. Data similarity focuses on the structural composition of the files and disregards the meaningful information they may carry. It can be used to identify duplicate or near-duplicate files and evaluate the efficiency of data compression or encryption algorithms.</p>"},{"location":"similarity/#2-content-similarity","title":"2. Content Similarity","text":"<p>Content Similarity:</p> <p>The measure of likeness between two digital media files, considering the perceptual, structural, and syntactic aspects of the decoded content, without necessarily considering the high-level understanding of the concepts represented.</p> <p>This category addresses the perceptual, structural, and syntactic similarity of digital media files after they have been decoded. Content similarity examines the likeness of the information presented, such as visual or auditory features, and takes into account the organization and presentation of the data. This type of similarity is useful for tasks like content-based retrieval, image or video classification, and multimedia summarization.</p>"},{"location":"similarity/#3-semantic-similarity","title":"3. Semantic Similarity","text":"<p>Semantic Similarity:</p> <p>The measure of likeness between two digital media files, based on the high-level understanding of the concepts, ideas, and context they represent, transcending the perceptual and structural aspects of the content.</p> <p>This form of similarity pertains to the high-level understanding of concepts conveyed by digital media files. Semantic similarity compares the meaning and context of the content, going beyond the perceptual and structural aspects. It is used in applications like natural language processing, knowledge representation, and semantic search.</p>"},{"location":"similarity/#limitations","title":"Limitations","text":"<p>While these three categories cover a comprehensive range of digital content similarity, there may be other specific types of similarity depending on the domain or application. For instance, some scenarios might require a focus on stylistic similarity, which compares the style or artistic attributes of media files, or functional similarity, which assesses how similar the intended purpose or function of the content is.</p>"},{"location":"similarity/#granularity","title":"Granularity","text":"<p>Global similarity and partial similarity are two approaches used to compare and analyze the likeness between digital content. These methods have different implications depending on the context and the nature of the data being compared.</p>"},{"location":"similarity/#global-similarity","title":"Global Similarity","text":"<p>This type of similarity considers the overall likeness between two digital content pieces in their entirety. It measures the extent to which the entire content of one piece matches or resembles the other, taking into account all aspects of the content, such as structure, syntax, and semantics. Global similarity is useful for tasks like identifying duplicate content, detecting plagiarism, or comparing entire documents, images, or audio files.</p> <p>Some challenges and intricacies associated with global similarity include:</p> <ul> <li>Sensitivity to minor differences: Small variations in content can lead to a significant   reduction in similarity scores, even if the overall content is largely similar.</li> <li>Scale and proportion: Differences in the scale, size, or proportion of elements within the   content can affect global similarity, even if the elements themselves are similar.</li> <li>Alignment: Misalignments or differences in the arrangement of content can impact global   similarity, even if the content is otherwise highly similar.</li> </ul>"},{"location":"similarity/#partial-similarity","title":"Partial Similarity","text":"<p>Partial similarity focuses on the similarity between specific segments or regions within the digital content, rather than comparing the content in its entirety. Partial similarity is useful for tasks like detecting recurring patterns, identifying similar substructures, or comparing specific parts of documents, images, or audio files.</p> <p>Some intricacies and challenges associated with partial similarity include:</p> <ul> <li>Identifying relevant segments: To effectively compare partial similarities, it is crucial to   identify and isolate the relevant segments or regions within the content. This can be challenging,   especially in cases where the boundaries are not clearly defined or are ambiguous.</li> <li>Varying granularity: The level of granularity at which partial similarity is assessed can   impact the results. Finer granularity may reveal subtle similarities, while coarser granularity   may emphasize broader patterns.</li> <li>Computational complexity: Comparing partial similarity can be computationally intensive, as it   requires analyzing and comparing multiple segments or regions within the content.</li> </ul> <p>Both global and partial similarity have their strengths and limitations, and the choice between them depends on the specific requirements of the task at hand. In some cases, a combination of the two approaches may be employed to achieve a more comprehensive understanding of the similarity in digital content.</p>"}]}